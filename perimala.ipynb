{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11342961,"sourceType":"datasetVersion","datasetId":7096834},{"sourceId":11344861,"sourceType":"datasetVersion","datasetId":7098349},{"sourceId":11398412,"sourceType":"datasetVersion","datasetId":7138811}],"dockerImageVersionId":31013,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset Loading","metadata":{}},{"cell_type":"code","source":"import os\n\nbase_path = \"/kaggle/input\"\n\nprint(\"Available datasets in /kaggle/input:\")\nprint(os.listdir(base_path))\n\n# Replace with your actual dataset folder name\ndata_path = \"/kaggle/input/sketches-png-dataset\"\n\n# Show top-level contents of the folder\nprint(\"Files/Folders in dataset directory:\")\nprint(os.listdir(data_path))\n\nprint(os.listdir(\"/kaggle/input/sketches-png-dataset\"))\n\ndata_path = \"/kaggle/input/sketches-png-dataset\"\n\n# List first-level folders (object categories?)\nprint(\"Top-level folders:\")\nprint(os.listdir(data_path))\n\n# Look inside one of the folders (e.g., apple)\nsample_class = os.listdir(data_path)[0]\nprint(f\"\\nFiles inside '{sample_class}':\")\nprint(os.listdir(os.path.join(data_path, sample_class))[:5])  # show first 5 images","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:56:33.096316Z","iopub.execute_input":"2025-04-14T16:56:33.096607Z","iopub.status.idle":"2025-04-14T16:56:33.127896Z","shell.execute_reply.started":"2025-04-14T16:56:33.096584Z","shell.execute_reply":"2025-04-14T16:56:33.127102Z"}},"outputs":[{"name":"stdout","text":"Available datasets in /kaggle/input:\n['sketch-features', 'sketches-png-dataset']\nFiles/Folders in dataset directory:\n['png']\n['png']\nTop-level folders:\n['png']\n\nFiles inside 'png':\n['fan', 'grenade', 'moon', 'microphone', 'calculator']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndata_path = \"/kaggle/input/sketches-png-dataset/png\"\n\n# List of object categories\ncategories = os.listdir(data_path)\n\n# How many categories and images to show\nnum_categories = 3\nimages_per_category = 3\n\nplt.figure(figsize=(images_per_category * 3, num_categories * 3))\n\nimg_count = 1\n\nfor category in categories[:num_categories]:\n    category_path = os.path.join(data_path, category)\n    \n    # Only continue if it's a folder\n    if not os.path.isdir(category_path):\n        continue\n    \n    images = os.listdir(category_path)[:images_per_category]\n\n    for img_file in images:\n        img_path = os.path.join(category_path, img_file)\n        try:\n            img = Image.open(img_path)\n            plt.subplot(num_categories, images_per_category, img_count)\n            plt.imshow(img, cmap='gray')\n            plt.title(category)\n            plt.axis('off')\n            img_count += 1\n        except:\n            print(f\"Could not open image: {img_path}\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\n\ndata_path = \"/kaggle/input/sketches-png-dataset/png\"\nimg_size = 128  # Resize all images to 128x128\nX = []\ny = []\n\ncategories = os.listdir(data_path)\n\nfor label, category in enumerate(categories):\n    category_path = os.path.join(data_path, category)\n    if not os.path.isdir(category_path):\n        continue\n    \n    for img_file in os.listdir(category_path):\n        img_path = os.path.join(category_path, img_file)\n        try:\n            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n            img = cv2.resize(img, (img_size, img_size))        # Resize\n            img = img / 255.0                                   # Normalize to [0,1]\n            X.append(img)\n            y.append(label)\n        except:\n            print(f\"Failed to process: {img_path}\")\n\nX = np.array(X)\ny = np.array(y)\nprint(\"hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:05:47.932711Z","iopub.execute_input":"2025-04-14T16:05:47.933194Z","iopub.status.idle":"2025-04-14T16:07:50.672154Z","shell.execute_reply.started":"2025-04-14T16:05:47.933171Z","shell.execute_reply":"2025-04-14T16:07:50.671370Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# 1. Denoising\ndef denoise_image(image):\n    return cv2.fastNlMeansDenoising((image * 255).astype(np.uint8), None, h=10, templateWindowSize=7, searchWindowSize=21) / 255.0\n\n# 2. Contrast Enhancement using Histogram Equalization\ndef enhance_contrast(image):\n    image_uint8 = (image * 255).astype(np.uint8)\n    return cv2.equalizeHist(image_uint8) / 255.0\n\n# 3. Apply a Filter (e.g., Sharpening)\ndef apply_filter(image):\n    kernel = np.array([[0, -1, 0],\n                       [-1, 5,-1],\n                       [0, -1, 0]])\n    return cv2.filter2D(image, -1, kernel)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.776235Z","iopub.status.idle":"2025-04-14T16:02:58.776545Z","shell.execute_reply.started":"2025-04-14T16:02:58.776393Z","shell.execute_reply":"2025-04-14T16:02:58.776407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\n\nX_preprocessed = []\nfor img in tqdm(X, desc=\"Preprocessing images\"):\n    denoised = denoise_image(img)\n    enhanced = enhance_contrast(denoised)\n    filtered = apply_filter(enhanced)\n    X_preprocessed.append(filtered)\n\nX_preprocessed = np.array(X_preprocessed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.778088Z","iopub.status.idle":"2025-04-14T16:02:58.778407Z","shell.execute_reply.started":"2025-04-14T16:02:58.778236Z","shell.execute_reply":"2025-04-14T16:02:58.778250Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualising pre-processed data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Display first 5 images from the preprocessed list\nplt.figure(figsize=(12, 4))\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(X_preprocessed[i], cmap='gray')\n    plt.title(f'Image {i+1}')\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.779348Z","iopub.status.idle":"2025-04-14T16:02:58.779571Z","shell.execute_reply.started":"2025-04-14T16:02:58.779475Z","shell.execute_reply":"2025-04-14T16:02:58.779483Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"!pip install scikit-image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.780652Z","iopub.status.idle":"2025-04-14T16:02:58.780912Z","shell.execute_reply.started":"2025-04-14T16:02:58.780775Z","shell.execute_reply":"2025-04-14T16:02:58.780789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"HOG Features","metadata":{}},{"cell_type":"code","source":"from skimage.feature import hog\n\n# Extract HOG features from preprocessed images\nhog_features = []\n\n# Parameters: adjust based on dataset complexity and image size\nhog_params = {\n    'orientations': 9,\n    'pixels_per_cell': (8, 8),\n    'cells_per_block': (2, 2),\n    'block_norm': 'L2-Hys'\n}\n\nfor img in X_preprocessed:\n    features = hog(img, **hog_params)\n    hog_features.append(features)\n\nhog_features = np.array(hog_features)\nprint(\"HOG feature shape:\", hog_features.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.781966Z","iopub.status.idle":"2025-04-14T16:02:58.782229Z","shell.execute_reply.started":"2025-04-14T16:02:58.782114Z","shell.execute_reply":"2025-04-14T16:02:58.782126Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CNN Features","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision\nprint(\"!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.782911Z","iopub.status.idle":"2025-04-14T16:02:58.783115Z","shell.execute_reply.started":"2025-04-14T16:02:58.783020Z","shell.execute_reply":"2025-04-14T16:02:58.783028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom tqdm import tqdm  # Progress bar\n\n# Define the transform: resize to 224x224 and normalize for ImageNet\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n                         std=[0.229, 0.224, 0.225])   # ImageNet std\n])\n\nprint(\"123\")\n\n# Load pretrained ResNet-34 model\nresnet = models.resnet34(pretrained=True)\nresnet.eval()\nprint(\"123\")\n\n\n# Remove the final fully connected layer to get features\nresnet_feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Output shape: (batch, 512, 1, 1)\n\nprint(\"123\")\n\n# Convert preprocessed images to CNN features\ncnn_features = []\n\nfor img in tqdm(X_preprocessed, desc=\"Extracting CNN features\"):\n    # Ensure image is in uint8 format (needed for PIL)\n    img_tensor = transform((img * 255).astype(np.uint8))\n    img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n\n    with torch.no_grad():\n        feat = resnet_feature_extractor(img_tensor)\n        feat = feat.view(-1).numpy()  # Flatten to 1D feature vector\n        cnn_features.append(feat)\n\n# Convert list to numpy array\ncnn_features = np.array(cnn_features)\nprint(\"CNN feature shape:\", cnn_features.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.783759Z","iopub.status.idle":"2025-04-14T16:02:58.783958Z","shell.execute_reply.started":"2025-04-14T16:02:58.783866Z","shell.execute_reply":"2025-04-14T16:02:58.783875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LBP Features","metadata":{}},{"cell_type":"code","source":"from skimage.feature import local_binary_pattern\nfrom tqdm import tqdm\n\n# Parameters for LBP\nradius = 1\nn_points = 8 * radius\nmethod = 'uniform'  # options: 'default', 'ror', 'uniform', 'var'\n\ndef compute_lbp_fast(img):\n    lbp = local_binary_pattern(img, n_points, radius, method)\n    # Use histogram to reduce feature size\n    (hist, _) = np.histogram(lbp.ravel(),\n                             bins=np.arange(0, n_points + 3),\n                             range=(0, n_points + 2))\n    hist = hist.astype(\"float\")\n    hist /= (hist.sum() + 1e-6)  # Normalize histogram\n    return hist\n\n# Apply to all images\nlbp_features_fast = np.array([compute_lbp_fast((img * 255).astype(np.uint8)) for img in tqdm(X_preprocessed, desc=\"Fast LBP\")])\nprint(\"Fast LBP feature shape:\", lbp_features_fast.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.785114Z","iopub.status.idle":"2025-04-14T16:02:58.785435Z","shell.execute_reply.started":"2025-04-14T16:02:58.785258Z","shell.execute_reply":"2025-04-14T16:02:58.785269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Saving Features as Pickle files","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Save HOG features\nwith open(\"hog_features.pkl\", \"wb\") as f:\n    pickle.dump(hog_features, f)\n\n# Save CNN features\nwith open(\"cnn_features.pkl\", \"wb\") as f:\n    pickle.dump(cnn_features, f)\n\n# Save LBP features\nwith open(\"lbp_features.pkl\", \"wb\") as f:\n    pickle.dump(lbp_features_fast, f)\n\nprint(\"All feature files saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading features from pickle files","metadata":{}},{"cell_type":"code","source":"import pickle\nimport numpy as np\n\n# Path to your uploaded dataset\nbase_path = \"/kaggle/input/sketch-features/\"\n\n# Load HOG features\nwith open(base_path + \"hog_features.pkl\", \"rb\") as f:\n    hog_features = pickle.load(f)\n\n# Load LBP features\nwith open(base_path + \"lbp_features.pkl\", \"rb\") as f:\n    lbp_features = pickle.load(f)\n\n# Load CNN features\nwith open(base_path + \"cnn_features.pkl\", \"rb\") as f:\n    cnn_features = pickle.load(f)\n\nprint(\"Shapes:\")\nprint(\"HOG:\", hog_features.shape)\nprint(\"LBP:\", lbp_features.shape)\nprint(\"CNN:\", cnn_features.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:07:53.856109Z","iopub.execute_input":"2025-04-14T16:07:53.856403Z","iopub.status.idle":"2025-04-14T16:07:56.637262Z","shell.execute_reply.started":"2025-04-14T16:07:53.856383Z","shell.execute_reply":"2025-04-14T16:07:56.636500Z"}},"outputs":[{"name":"stdout","text":"Shapes:\nHOG: (20000, 8100)\nLBP: (20000, 10)\nCNN: (20000, 512)\nCombined feature shape: (20000, 522)\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"Standardizing features and combining them ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Standardize the features separately\nscaler_hog = StandardScaler()\nscaler_lbp = StandardScaler()\nscaler_cnn = StandardScaler()\n\nhog_scaled = scaler_hog.fit_transform(hog_features)\nlbp_scaled = scaler_lbp.fit_transform(lbp_features)\ncnn_scaled = scaler_cnn.fit_transform(cnn_features)\n\n# Combine all features\ncombined_features = np.concatenate([lbp_scaled, cnn_scaled], axis=1)\nprint(\"Combined feature shape:\", combined_features.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_labels = sorted(os.listdir(data_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:08:03.535317Z","iopub.execute_input":"2025-04-14T16:08:03.535610Z","iopub.status.idle":"2025-04-14T16:08:03.539761Z","shell.execute_reply.started":"2025-04-14T16:08:03.535591Z","shell.execute_reply":"2025-04-14T16:08:03.539013Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"# PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport joblib\nfrom sklearn.model_selection import train_test_split\n# Apply PCA to retain 95% variance\npca = PCA(n_components=0.95, random_state=42)\nX_pca = pca.fit_transform(combined_features)\n\n# Split and re-train (example with Logistic Regression)\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:08:06.004510Z","iopub.execute_input":"2025-04-14T16:08:06.005290Z","iopub.status.idle":"2025-04-14T16:08:07.916751Z","shell.execute_reply.started":"2025-04-14T16:08:06.005265Z","shell.execute_reply":"2025-04-14T16:08:07.916114Z"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"# Model Implementation with PCA","metadata":{}},{"cell_type":"markdown","source":"KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split the data\n# X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n\n# Train KNN\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train_pca, y_train_pca)\n\n# Predict and evaluate\ny_pred = knn.predict(X_test_pca)\n\nprint(\"KNN Accuracy:\", accuracy_score(y_test_pca, y_pred))\n\n# import joblib\n# joblib.dump(knn_pca, 'knn_model_pca.pkl')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:08:11.750699Z","iopub.execute_input":"2025-04-14T16:08:11.750961Z","iopub.status.idle":"2025-04-14T16:08:12.297514Z","shell.execute_reply.started":"2025-04-14T16:08:11.750943Z","shell.execute_reply":"2025-04-14T16:08:12.296855Z"}},"outputs":[{"name":"stdout","text":"KNN Accuracy: 0.2685\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport joblib\n\n# # Split the data\n# X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n\n# Train the Random Forest classifier\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train_pca, y_train_pca)\n\n# Fast batch prediction\ny_pred_rf = rf_clf.predict(X_test_pca)\n\n# Evaluate accuracy\nprint(\"Random Forest Accuracy (Combined Features):\", accuracy_score(y_test_pca, y_pred_rf))\n# joblib.dump(rf_clf, 'random_forest_model.pkl')\n# import joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:08:19.021863Z","iopub.execute_input":"2025-04-14T16:08:19.022118Z","iopub.status.idle":"2025-04-14T16:12:07.445661Z","shell.execute_reply.started":"2025-04-14T16:08:19.022099Z","shell.execute_reply":"2025-04-14T16:12:07.444747Z"}},"outputs":[{"name":"stdout","text":"Random Forest Accuracy (Combined Features): 0.27025\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"Logistic Regression","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\n# # Split\n# X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n\n# Train\nlr_clf = LogisticRegression(max_iter=1000)\nlr_clf.fit(X_train_pca, y_train_pca)\n\n# Predict with tqdm\ny_pred = []\nfor sample in tqdm(X_test_pca, desc=\"Predicting with Logistic Regression\"):\n    pred = lr_clf.predict(sample.reshape(1, -1))\n    y_pred.append(pred[0])\n\n# Accuracy\nprint(\"Logistic Regression Accuracy (Combined):\", accuracy_score(y_test_pca, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:12:14.753786Z","iopub.execute_input":"2025-04-14T16:12:14.754056Z","iopub.status.idle":"2025-04-14T16:15:26.822143Z","shell.execute_reply.started":"2025-04-14T16:12:14.754038Z","shell.execute_reply":"2025-04-14T16:15:26.821457Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nPredicting with Logistic Regression: 100%|██████████| 4000/4000 [00:00<00:00, 9105.89it/s] ","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy (Combined): 0.44075\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\n# Train the SVM classifier (RBF kernel by default)\nsvm_clf = SVC(kernel='rbf', probability=False)  # You can also try 'linear' or 'poly'\nsvm_clf.fit(X_train_pca, y_train_pca)\n\n# Predict with tqdm progress bar\ny_pred_svm = []\nfor sample in tqdm(X_test_pca, desc=\"Predicting with SVM\", unit=\"sample\"):\n    pred = svm_clf.predict(sample.reshape(1, -1))\n    y_pred_svm.append(pred[0])\n\n# Evaluate accuracy\nprint(\"SVM Accuracy (Combined Features):\", accuracy_score(y_test_pca, y_pred_svm))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:15:54.044176Z","iopub.execute_input":"2025-04-14T16:15:54.044862Z","iopub.status.idle":"2025-04-14T16:17:15.608050Z","shell.execute_reply.started":"2025-04-14T16:15:54.044834Z","shell.execute_reply":"2025-04-14T16:17:15.607377Z"}},"outputs":[{"name":"stderr","text":"Predicting with SVM: 100%|██████████| 4000/4000 [00:46<00:00, 85.84sample/s]","output_type":"stream"},{"name":"stdout","text":"SVM Accuracy (Combined Features): 0.432\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"MLP","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom tqdm import tqdm\n\nmlp = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=1, warm_start=True, random_state=42)\n\nn_epochs = 100\nfor _ in tqdm(range(n_epochs), desc=\"Training MLP\"):\n    mlp.fit(X_train_pca, y_train_pca)\n\ny_pred_mlp = mlp.predict(X_test_pca)\nprint(\"MLP Accuracy:\", accuracy_score(y_test_pca, y_pred_mlp))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:17:50.335234Z","iopub.execute_input":"2025-04-14T16:17:50.335587Z","iopub.status.idle":"2025-04-14T16:18:39.889158Z","shell.execute_reply.started":"2025-04-14T16:17:50.335564Z","shell.execute_reply":"2025-04-14T16:18:39.888408Z"}},"outputs":[{"name":"stderr","text":"Training MLP:   0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n  warnings.warn(\nTraining MLP: 100%|██████████| 100/100 [00:49<00:00,  2.02it/s]","output_type":"stream"},{"name":"stdout","text":"MLP Accuracy: 0.4085\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\n# Initialize Naive Bayes model\nnb_clf = GaussianNB()\n\n# Split\n# X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n\n# Train the model\nnb_clf.fit(X_train_pca, y_train_pca)\n\n# Predict with tqdm progress bar\ny_pred_nb = []\nfor sample in tqdm(X_test_pca, desc=\"Predicting with Naive Bayes\", unit=\"sample\"):\n    pred = nb_clf.predict(sample.reshape(1, -1))\n    y_pred_nb.append(pred[0])\n\n# Evaluate accuracy\nprint(\"Naive Bayes Accuracy (Combined Features):\", accuracy_score(y_test_pca, y_pred_nb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:19:21.457149Z","iopub.execute_input":"2025-04-14T16:19:21.457506Z","iopub.status.idle":"2025-04-14T16:19:43.746857Z","shell.execute_reply.started":"2025-04-14T16:19:21.457486Z","shell.execute_reply":"2025-04-14T16:19:43.746209Z"}},"outputs":[{"name":"stderr","text":"Predicting with Naive Bayes: 100%|██████████| 4000/4000 [00:22<00:00, 180.04sample/s]","output_type":"stream"},{"name":"stdout","text":"Naive Bayes Accuracy (Combined Features): 0.3465\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"# Model Implementation without PCA","metadata":{}},{"cell_type":"markdown","source":"Logistic Regression","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n\n# Train\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\n# Predict with tqdm\ny_pred_lr = []\nfor sample in tqdm(X_test, desc=\"Predicting with Logistic Regression\"):\n    pred = clf.predict(sample.reshape(1, -1))\n    y_pred_lr.append(pred[0])\n\n# Accuracy\nprint(\"Logistic Regression Accuracy (Combined):\", accuracy_score(y_test, y_pred_lr))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:19:47.321630Z","iopub.execute_input":"2025-04-14T16:19:47.322359Z","iopub.status.idle":"2025-04-14T16:23:08.677965Z","shell.execute_reply.started":"2025-04-14T16:19:47.322311Z","shell.execute_reply":"2025-04-14T16:23:08.677258Z"}},"outputs":[{"name":"stderr","text":"Predicting with Logistic Regression: 100%|██████████| 4000/4000 [00:00<00:00, 7926.48it/s]","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Accuracy (Combined): 0.47675\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\n# Train the SVM classifier (RBF kernel by default)\nsvm_clf_p = SVC(kernel='rbf', probability=False)  # You can also try 'linear' or 'poly'\nsvm_clf_p.fit(X_train, y_train)\n\n# Predict with tqdm progress bar\ny_pred_svm = []\nfor sample in tqdm(X_test, desc=\"Predicting with SVM\", unit=\"sample\"):\n    pred = svm_clf_p.predict(sample.reshape(1, -1))\n    y_pred_svm.append(pred[0])\n\n# Evaluate accuracy\nprint(\"SVM Accuracy (Combined Features):\", accuracy_score(y_test, y_pred_svm))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:24:39.159313Z","iopub.execute_input":"2025-04-14T16:24:39.159896Z","iopub.status.idle":"2025-04-14T16:26:35.278593Z","shell.execute_reply.started":"2025-04-14T16:24:39.159875Z","shell.execute_reply":"2025-04-14T16:26:35.277819Z"}},"outputs":[{"name":"stderr","text":"Predicting with SVM: 100%|██████████| 4000/4000 [01:02<00:00, 64.45sample/s]","output_type":"stream"},{"name":"stdout","text":"SVM Accuracy (Combined Features): 0.4365\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport joblib\n\n# # Split the data\n# X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n\n# Train the Random Forest classifier\nrf_clf_p = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf_p.fit(X_train, y_train)\n\n# Fast batch prediction\ny_pred_rf = rf_clf_p.predict(X_test)\n\n# Evaluate accuracy\nprint(\"Random Forest Accuracy (Combined Features):\", accuracy_score(y_test, y_pred_rf))\n# joblib.dump(rf_clf, 'random_forest_model.pkl')\n# import joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:26:51.368055Z","iopub.execute_input":"2025-04-14T16:26:51.368613Z","iopub.status.idle":"2025-04-14T16:31:57.077197Z","shell.execute_reply.started":"2025-04-14T16:26:51.368590Z","shell.execute_reply":"2025-04-14T16:31:57.076210Z"}},"outputs":[{"name":"stdout","text":"Random Forest Accuracy (Combined Features): 0.32175\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split the data\n# X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n\n# Train KNN\nknn_p = KNeighborsClassifier(n_neighbors=3)\nknn_p.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = knn_p.predict(X_test)\n\nprint(\"KNN Accuracy:\", accuracy_score(y_test, y_pred))\n\n# import joblib\n# joblib.dump(knn_pca, 'knn_model_pca.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.808027Z","iopub.status.idle":"2025-04-14T16:02:58.808279Z","shell.execute_reply.started":"2025-04-14T16:02:58.808150Z","shell.execute_reply":"2025-04-14T16:02:58.808160Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"MLP","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom tqdm import tqdm\n\nmlp_p = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=1, warm_start=True, random_state=42)\n\nn_epochs = 100\nfor _ in tqdm(range(n_epochs), desc=\"Training MLP\"):\n    mlp_p.fit(X_train, y_train)\n\ny_pred_mlp = mlp_p.predict(X_test)\nprint(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.809734Z","iopub.status.idle":"2025-04-14T16:02:58.810980Z","shell.execute_reply.started":"2025-04-14T16:02:58.810814Z","shell.execute_reply":"2025-04-14T16:02:58.810826Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\n# Initialize Naive Bayes model\nnb_clf_p = GaussianNB()\n\n# Split\n# X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n\n# Train the model\nnb_clf_p.fit(X_train, y_train)\n\n# Predict with tqdm progress bar\ny_pred_nb = []\nfor sample in tqdm(X_test, desc=\"Predicting with Naive Bayes\", unit=\"sample\"):\n    pred = nb_clf_p.predict(sample.reshape(1, -1))\n    y_pred_nb.append(pred[0])\n\n# Evaluate accuracy\nprint(\"Naive Bayes Accuracy (Combined Features):\", accuracy_score(y_test, y_pred_nb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.813024Z","iopub.status.idle":"2025-04-14T16:02:58.813283Z","shell.execute_reply.started":"2025-04-14T16:02:58.813156Z","shell.execute_reply":"2025-04-14T16:02:58.813171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TSNE visualisation of features","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Reduce dimensionality for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\nfeatures_2d = tsne.fit_transform(combined_features)\n\n# Plot\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=y, cmap='tab10', s=10, alpha=0.7)\nplt.title(\"t-SNE Visualization of Combined Features\")\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.colorbar(scatter, label=\"Class Label\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.814016Z","iopub.status.idle":"2025-04-14T16:02:58.814277Z","shell.execute_reply.started":"2025-04-14T16:02:58.814150Z","shell.execute_reply":"2025-04-14T16:02:58.814164Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Accuracy Plot","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Store accuracies\naccuracies = {\n    \"KNN\": accuracy_score(y_test, knn_p.predict(X_test)),\n    \"Random Forest\": accuracy_score(y_test, rf_clf_p.predict(X_test)),\n    \"LogReg\": accuracy_score(y_test, y_pred_lr),\n    \"MLP\": accuracy_score(y_test, y_pred_mlp),\n    \"Naive Bayes\": accuracy_score(y_test_pca, y_pred_nb),\n    \"SVM\": accuracy_score(y_test, y_pred_svm),\n}\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.bar(accuracies.keys(), accuracies.values(), color=['skyblue', 'lightgreen', 'salmon'])\nplt.ylabel(\"Accuracy\")\nplt.title(\"Model Accuracy Comparison\")\nplt.ylim(0, 1)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.815490Z","iopub.status.idle":"2025-04-14T16:02:58.815736Z","shell.execute_reply.started":"2025-04-14T16:02:58.815617Z","shell.execute_reply":"2025-04-14T16:02:58.815629Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Precision, Recall, F1 score Plot","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision = {\n    \"KNN\": precision_score(y_test, knn_p.predict(X_test), average='macro'),\n    \"Random Forest\": precision_score(y_test, rf_clf_p.predict(X_test), average='macro'),\n    \"LogReg\": precision_score(y_test, y_pred_lr, average='macro'),\n    \"MLP\": precision_score(y_test, y_pred_mlp, average='macro'),\n    \"Naive Bayes\": precision_score(y_test_pca, y_pred_nb, average='macro'),\n    \"SVM\": precision_score(y_test, y_pred_svm, average='macro'),\n}\n\nrecall = {\n    \"KNN\": recall_score(y_test, knn_p.predict(X_test), average='macro'),\n    \"Random Forest\": recall_score(y_test, rf_clf_p.predict(X_test), average='macro'),\n    \"LogReg\": recall_score(y_test, y_pred_lr, average='macro'),\n    \"MLP\": recall_score(y_test, y_pred_mlp, average='macro'),\n    \"Naive Bayes\": recall_score(y_test_pca, y_pred_nb, average='macro'),\n    \"SVM\": recall_score(y_test, y_pred_svm, average='macro'),\n}\n\nf1 = {\n    \"KNN\": f1_score(y_test, knn_p.predict(X_test), average='macro'),\n    \"Random Forest\": f1_score(y_test, rf_clf_p.predict(X_test), average='macro'),\n    \"LogReg\": f1_score(y_test, y_pred_lr, average='macro'),\n    \"MLP\": f1_score(y_test, y_pred_mlp, average='macro'),\n    \"Naive Bayes\": f1_score(y_test_pca, y_pred_nb, average='macro'),\n    \"SVM\": f1_score(y_test, y_pred_svm, average='macro'),\n}\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.817589Z","iopub.status.idle":"2025-04-14T16:02:58.818049Z","shell.execute_reply.started":"2025-04-14T16:02:58.817885Z","shell.execute_reply":"2025-04-14T16:02:58.817899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# X-axis: model names\nmodels = list(precision.keys())\n\n# Y-axis values\nprecision_scores = [precision[m] for m in models]\nrecall_scores = [recall[m] for m in models]\nf1_scores = [f1[m] for m in models]\n\n# Positioning\nx = np.arange(len(models))\nwidth = 0.25\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.bar(x - width, precision_scores, width, label='Precision', color='skyblue')\nplt.bar(x, recall_scores, width, label='Recall', color='orange')\nplt.bar(x + width, f1_scores, width, label='F1 Score', color='green')\n\n# Labels\nplt.xticks(x, models)\nplt.ylabel(\"Score\")\nplt.ylim(0, 1)\nplt.title(\"Precision, Recall, and F1 Score Comparison\")\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.819065Z","iopub.status.idle":"2025-04-14T16:02:58.819401Z","shell.execute_reply.started":"2025-04-14T16:02:58.819219Z","shell.execute_reply":"2025-04-14T16:02:58.819233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving Models as Pickle files","metadata":{}},{"cell_type":"code","source":"import joblib\n\njoblib.dump(knn, 'knn_model.pkl')\njoblib.dump(clf, 'logistic_regression_model.pkl')\njoblib.dump(rf_clf, 'random_forest_model.pkl')\njoblib.dump(svm_clf, 'svm_model.pkl')\njoblib.dump(nb_clf, 'naive_bayes_model.pkl')\njoblib.dump(mlp_clf, 'mlp_model.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:02:58.828523Z","iopub.status.idle":"2025-04-14T16:02:58.828764Z","shell.execute_reply.started":"2025-04-14T16:02:58.828622Z","shell.execute_reply":"2025-04-14T16:02:58.828630Z"}},"outputs":[],"execution_count":null}]}